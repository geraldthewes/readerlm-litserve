# ReaderLM LitServe Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# Model Configuration
# =============================================================================

# Hugging Face model name/path for the reader LM model
MODEL_NAME=jinaai/ReaderLM-v2

# Model revision (commit hash, tag, or branch) for reproducibility
# Use a specific commit hash for production deployments
MODEL_REVISION=main

# =============================================================================
# Generation Parameters
# =============================================================================

# Maximum number of new tokens to generate
MAX_NEW_TOKENS=1024

# Sampling temperature (0 = deterministic, recommended for ReaderLM-v2)
TEMPERATURE=0

# Repetition penalty to reduce repetitive text (1.0 = no penalty)
REPETITION_PENALTY=1.08

# =============================================================================
# Server Configuration
# =============================================================================

# Port the server will listen on
SERVER_PORT=8000

# =============================================================================
# Client Configuration (for client.py)
# =============================================================================

# Base URL of the server
SERVER_URL=http://127.0.0.1:8000

# Request timeout in seconds (model inference can be slow)
REQUEST_TIMEOUT=120
