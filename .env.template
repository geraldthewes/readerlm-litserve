# ReaderLM LitServe Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# Model Configuration
# =============================================================================

# Hugging Face model name/path for the reader LM model
MODEL_NAME=jinaai/ReaderLM-v2

# Model revision (commit hash, tag, or branch) for reproducibility
# Use a specific commit hash for production deployments
MODEL_REVISION=main

# Model precision/dtype for inference
# Options: auto, float16, bfloat16, float32
# Use float16 for GPUs that don't support bfloat16 (e.g., T4)
# Use auto to let transformers choose based on model config
MODEL_DTYPE=auto

# Attention implementation for the model
# Options: eager, sdpa, flash_attention_2
# Use "eager" for better compatibility with float16 on older GPUs (Qwen2 recommendation)
# Use "sdpa" for PyTorch's native scaled dot-product attention
# Use "flash_attention_2" for Flash Attention 2 (requires compatible GPU)
ATTN_IMPLEMENTATION=eager

# =============================================================================
# Generation Parameters
# =============================================================================

# Maximum number of new tokens to generate
MAX_NEW_TOKENS=1024

# Sampling temperature (0 = deterministic, recommended for ReaderLM-v2)
TEMPERATURE=0

# Repetition penalty to reduce repetitive text (1.0 = no penalty)
REPETITION_PENALTY=1.08

# =============================================================================
# Quantization Configuration (VRAM Optimization)
# =============================================================================

# Quantization mode for VRAM reduction
# Options: none, 4bit, 8bit
# - none: Full precision (14-16 GB VRAM required)
# - 4bit: NF4 quantization (4-5 GB VRAM, recommended for limited VRAM)
# - 8bit: 8-bit quantization (7-8 GB VRAM)
QUANTIZATION_MODE=none

# 4-bit quantization type (only used when QUANTIZATION_MODE=4bit)
# Options: nf4, fp4
# - nf4: NormalFloat4 (recommended, better for weights)
# - fp4: 4-bit floating point
QUANTIZATION_TYPE=nf4

# Enable double quantization for additional VRAM savings (4bit only)
# Double quantization quantizes the quantization constants
USE_DOUBLE_QUANT=true

# =============================================================================
# HTML Preprocessing Configuration
# =============================================================================

# Use readability-lxml to extract main content before model inference
# This typically reduces input size by 50-80%, improving VRAM usage
USE_READABILITY=true

# Maximum input tokens per chunk (used for chunking large documents)
# Documents exceeding this limit will be split at natural boundaries
MAX_INPUT_TOKENS=8000

# Enable automatic chunking for large documents
# When enabled, large documents are split and processed in parts
ENABLE_CHUNKING=true

# =============================================================================
# Server Configuration
# =============================================================================

# Port the server will listen on
SERVER_PORT=8000

# =============================================================================
# URL Fetching Configuration (for GET /{url} endpoint)
# =============================================================================

# Timeout for fetching external URLs in seconds
URL_FETCH_TIMEOUT=30

# User-Agent header sent when fetching URLs
URL_FETCH_USER_AGENT=ReaderLM/1.0

# Enable SSRF protection (block requests to private IPs)
# Set to "false" only in trusted environments
BLOCK_PRIVATE_IPS=true

# Domain allowlist (comma-separated, empty = allow all)
# Example: example.com,wikipedia.org
ALLOWED_DOMAINS=

# Domain blocklist (comma-separated, empty = block none)
# Example: localhost,internal.corp
BLOCKED_DOMAINS=

# =============================================================================
# Client Configuration (for client.py)
# =============================================================================

# Base URL of the server
SERVER_URL=http://127.0.0.1:8000

# Request timeout in seconds (model inference can be slow)
REQUEST_TIMEOUT=120
