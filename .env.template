# ReaderLM LitServe Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# Model Configuration
# =============================================================================

# Hugging Face model name/path for the reader LM model
MODEL_NAME=jinaai/ReaderLM-v2

# Model revision (commit hash, tag, or branch) for reproducibility
# Use a specific commit hash for production deployments
MODEL_REVISION=main

# =============================================================================
# Generation Parameters
# =============================================================================

# Maximum number of new tokens to generate
MAX_NEW_TOKENS=1024

# Sampling temperature (0 = deterministic, recommended for ReaderLM-v2)
TEMPERATURE=0

# Repetition penalty to reduce repetitive text (1.0 = no penalty)
REPETITION_PENALTY=1.08

# =============================================================================
# Server Configuration
# =============================================================================

# Port the server will listen on
SERVER_PORT=8000

# =============================================================================
# URL Fetching Configuration (for GET /{url} endpoint)
# =============================================================================

# Timeout for fetching external URLs in seconds
URL_FETCH_TIMEOUT=30

# User-Agent header sent when fetching URLs
URL_FETCH_USER_AGENT=ReaderLM/1.0

# Enable SSRF protection (block requests to private IPs)
# Set to "false" only in trusted environments
BLOCK_PRIVATE_IPS=true

# Domain allowlist (comma-separated, empty = allow all)
# Example: example.com,wikipedia.org
ALLOWED_DOMAINS=

# Domain blocklist (comma-separated, empty = block none)
# Example: localhost,internal.corp
BLOCKED_DOMAINS=

# =============================================================================
# Client Configuration (for client.py)
# =============================================================================

# Base URL of the server
SERVER_URL=http://127.0.0.1:8000

# Request timeout in seconds (model inference can be slow)
REQUEST_TIMEOUT=120
